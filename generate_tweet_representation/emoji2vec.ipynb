{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Before We Start\n",
    "\n",
    "In this paper, to generate representation of an emoji, we use the idea proposed by Eisner, Ben, et al.2016. The link of the paper is given below:\n",
    "\n",
    "https://arxiv.org/pdf/1609.08359.pdf\n",
    "\n",
    "To run the code, please go to this paper's github page first and download that repository to a local path:\n",
    "\n",
    "https://github.com/uclmr/emoji2vec\n",
    "\n",
    "Then use the os package in python to change the current working directory to the path you have just specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = r'F:\\CityU\\Hong Kong Twitter 2016\\emoji2vec'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\CityU\\\\Hong Kong Twitter 2016\\\\emoji2vec'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Load some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commonly used\n",
    "import gensim.models as gs\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# This paper requires\n",
    "import twitter_sentiment_dataset as tsd\n",
    "import phrase2vec as p2v\n",
    "from twitter_sentiment_dataset import TweetTrainingExample\n",
    "from model import ModelParams\n",
    "\n",
    "# tokenization\n",
    "import nltk.tokenize as tk\n",
    "\n",
    "# Neglect some warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_path='./data/word2vec/'\n",
    "\n",
    "in_dim = 100   # Length of word2vec vectors\n",
    "out_dim = 100  # Desired dimension of output vectors\n",
    "pos_ex = 4\n",
    "neg_ratio = 1\n",
    "max_epochs = 40\n",
    "dropout = 0.1\n",
    "\n",
    "params = ModelParams(in_dim=in_dim, out_dim=out_dim, pos_ex=pos_ex, max_epochs=max_epochs,\n",
    "                    neg_ratio=neg_ratio, learning_rate=0.001, dropout=dropout, class_threshold=0.5)\n",
    "\n",
    "\n",
    "\n",
    "e2v_ours_path = params.model_folder('unicode') + '/emoji2vec_100.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load the Pre-trained Word Vectors and Emoji Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./results/unicode/k-100_pos-4_rat-1_ep-40_dr-1/emoji2vec_100.bin'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2v_ours_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The fasttext_model is the model pretrained from sentiment140\n",
    "w2v = gs.FastText.load(os.path.join(w2v_path, 'fasttext_model'))\n",
    "e2v_ours = gs.KeyedVectors.load_word2vec_format(e2v_ours_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Combine the word embedding and emoji embedding together\n",
    "p2v_our_emoji = p2v.Phrase2Vec(out_dim, w2v, e2v=e2v_ours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Generate Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataframe which contains processed tweets, geoinformation, etc\n",
    "tweet_2017_path = r'F:\\CityU\\Datasets\\Hong Kong Tweets 2017'\n",
    "final_zh_sample_cleaned_and_translated = pd.read_pickle(\n",
    "        os.path.join(tweet_2017_path, 'final_sample_cleaned_and_translated_2.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_zh_sample_cleaned_and_translated.loc[\n",
    "        final_zh_sample_cleaned_and_translated['cleaned_text'] == '', 'cleaned_text'] = \\\n",
    "        final_zh_sample_cleaned_and_translated['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_zh_sample_cleaned_and_translated = final_zh_sample_cleaned_and_translated[['user_id_str', 'cleaned_text', 'lang', 'lat', 'lon', 'month']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_str</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lang</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>378134755</td>\n",
       "      <td>taste food festival ü•ô üçª üçú ü•ô üçª üçú ü•ô üçª üçú ü•ô üçª üçú ce...</td>\n",
       "      <td>zh</td>\n",
       "      <td>22.283612</td>\n",
       "      <td>114.162818</td>\n",
       "      <td>Mar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80997783</td>\n",
       "      <td>üòã godiva chocolate ice cream holiday internati...</td>\n",
       "      <td>zh</td>\n",
       "      <td>22.315168</td>\n",
       "      <td>113.934905</td>\n",
       "      <td>Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>785638</td>\n",
       "      <td>because of the church band camp receive the ba...</td>\n",
       "      <td>zh</td>\n",
       "      <td>22.278070</td>\n",
       "      <td>114.184710</td>\n",
       "      <td>Aug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1473379776</td>\n",
       "      <td>if you want to leave you have to be refuel you...</td>\n",
       "      <td>zh</td>\n",
       "      <td>22.371072</td>\n",
       "      <td>114.111811</td>\n",
       "      <td>Nov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5417862</td>\n",
       "      <td>a big share in fact a few good food be expensi...</td>\n",
       "      <td>zh</td>\n",
       "      <td>22.294690</td>\n",
       "      <td>114.168125</td>\n",
       "      <td>May</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>53893147</td>\n",
       "      <td>three paste of brine clam meat and the classic...</td>\n",
       "      <td>zh</td>\n",
       "      <td>22.280250</td>\n",
       "      <td>114.158330</td>\n",
       "      <td>Feb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>213927190</td>\n",
       "      <td>enter the market</td>\n",
       "      <td>zh</td>\n",
       "      <td>22.285359</td>\n",
       "      <td>114.157874</td>\n",
       "      <td>Dec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2422020475</td>\n",
       "      <td>for six year i use to be the enemy of the left...</td>\n",
       "      <td>zh</td>\n",
       "      <td>22.333600</td>\n",
       "      <td>114.159000</td>\n",
       "      <td>Feb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28544855</td>\n",
       "      <td>memory repeat taste different now repeat victo...</td>\n",
       "      <td>zh</td>\n",
       "      <td>22.281611</td>\n",
       "      <td>114.188718</td>\n",
       "      <td>Jan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>53893147</td>\n",
       "      <td>hand hold with a hand so tasty üòã causeway poin...</td>\n",
       "      <td>zh</td>\n",
       "      <td>22.279230</td>\n",
       "      <td>114.181870</td>\n",
       "      <td>Aug</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id_str                                       cleaned_text lang  \\\n",
       "0    378134755  taste food festival ü•ô üçª üçú ü•ô üçª üçú ü•ô üçª üçú ü•ô üçª üçú ce...   zh   \n",
       "1     80997783  üòã godiva chocolate ice cream holiday internati...   zh   \n",
       "2       785638  because of the church band camp receive the ba...   zh   \n",
       "3   1473379776  if you want to leave you have to be refuel you...   zh   \n",
       "4      5417862  a big share in fact a few good food be expensi...   zh   \n",
       "5     53893147  three paste of brine clam meat and the classic...   zh   \n",
       "6    213927190                                   enter the market   zh   \n",
       "7   2422020475  for six year i use to be the enemy of the left...   zh   \n",
       "8     28544855  memory repeat taste different now repeat victo...   zh   \n",
       "9     53893147  hand hold with a hand so tasty üòã causeway poin...   zh   \n",
       "\n",
       "         lat         lon month  \n",
       "0  22.283612  114.162818   Mar  \n",
       "1  22.315168  113.934905   Jan  \n",
       "2  22.278070  114.184710   Aug  \n",
       "3  22.371072  114.111811   Nov  \n",
       "4  22.294690  114.168125   May  \n",
       "5  22.280250  114.158330   Feb  \n",
       "6  22.285359  114.157874   Dec  \n",
       "7  22.333600  114.159000   Feb  \n",
       "8  22.281611  114.188718   Jan  \n",
       "9  22.279230  114.181870   Aug  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_zh_sample_cleaned_and_translated.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886, 6)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_zh_sample_cleaned_and_translated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a list of all the tweets in this dataset\n",
    "sample_zh_tweets = list(final_zh_sample_cleaned_and_translated['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_tweet_vector_averages_for_prediction(tweets, p2v):\n",
    "    \"\"\"\n",
    "    Take the vector sum of all tokens in each tweet\n",
    "\n",
    "    Args:\n",
    "        tweets: All tweets\n",
    "        p2v: Phrase2Vec model\n",
    "\n",
    "    Returns:\n",
    "        Average vectors for each tweet\n",
    "    \"\"\"\n",
    "    tokenizer = tk.TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "\n",
    "    avg_vecs = list()\n",
    "\n",
    "    for tweet in tweets:\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        avg_vecs.append(np.sum([p2v[x] for x in tokens], axis=0) / len(tokens))\n",
    "\n",
    "    return avg_vecs\n",
    "\n",
    "\n",
    "def list_of_array_to_array(list_array):\n",
    "    \"\"\"\n",
    "    Transform a list of one-dimensional arrays to a numpy array\n",
    "    Args:\n",
    "        list_array: a list of arrays\n",
    "    \n",
    "    Returns:\n",
    "        A numpy array\n",
    "    \"\"\"\n",
    "    shape = list(list_array[0].shape)\n",
    "    shape[:0] = [len(list_array)]\n",
    "    arr = np.concatenate(list_array).reshape(shape)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_representations_zh_sample = prepare_tweet_vector_averages_for_prediction(sample_zh_tweets, p2v_our_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_representations_zh_sample_array = list_of_array_to_array(tweets_representations_zh_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.44123602, -0.09781744, -0.18259196, ..., -0.2746119 ,\n",
       "        -0.26236635, -0.47610205],\n",
       "       [ 0.00924975,  0.03655616, -0.55427706, ..., -0.10963333,\n",
       "         0.5381336 , -1.3387452 ],\n",
       "       [-1.0994582 , -0.7486428 , -0.96823746, ...,  0.48931405,\n",
       "        -0.8802878 , -1.2700877 ],\n",
       "       ...,\n",
       "       [-0.41797507, -0.18330587, -0.9719057 , ...,  0.16747317,\n",
       "        -0.81420517, -0.95749223],\n",
       "       [ 0.39502248,  0.11920539, -1.9165957 , ..., -0.40935957,\n",
       "        -0.6471186 , -1.3098006 ],\n",
       "       [-1.4964355 , -0.16747546, -1.0323752 , ...,  0.30381542,\n",
       "        -0.26012287, -1.1634724 ]], dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_representations_zh_sample_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(886, 100)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(tweets_representations_zh_sample_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
